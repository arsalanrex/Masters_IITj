{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-22T11:49:26.122979Z",
     "start_time": "2024-03-22T11:49:26.109544Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    itr     x1     x2                 tol\n",
      "0     0      2      2                None\n",
      "1     1  1.800  2.000   0.200000000000000\n",
      "2     2  1.640  2.000   0.160000610351563\n",
      "3     3  1.512  2.000   0.127999877929688\n",
      "4     4  1.410  2.000   0.102398681640625\n",
      "5     5  1.328  2.000  0.0819183349609376\n",
      "6     6  1.262  2.000  0.0655334472656250\n",
      "7     7  1.210  2.000  0.0524261474609375\n",
      "8     8  1.168  2.000  0.0419403076171876\n",
      "9     9  1.134  2.000  0.0335510253906250\n",
      "10   10  1.107  2.000  0.0268402099609375\n"
     ]
    }
   ],
   "source": [
    "# steepest decent method with two variables\n",
    "\n",
    "import sympy as sp\n",
    "import pandas as pd\n",
    "\n",
    "def steepest_descent(func, x, x0, step_size, tol, max_iter, precision):\n",
    "    \n",
    "    gradient = [sp.diff(func, var) for var in x]\n",
    "    \n",
    "    # Initialize iteration data list with initial assumption\n",
    "    iterations_data = [{\"itr\": 0, \"x1\": x0[0], \"x2\": x0[1], \"tol\": None}]\n",
    "\n",
    "    x_current = x0\n",
    "    iterations = 0\n",
    "\n",
    "    while iterations < max_iter:\n",
    "        gradient_at_x = [g.subs(zip(x, x_current)) for g in gradient]\n",
    "\n",
    "        # Update rule: x_next = x_current - step_size * gradient\n",
    "        x_next = [x_curr - step_size * grad for x_curr, grad in zip(x_current, gradient_at_x)]\n",
    "\n",
    "        # Calculate the Euclidean distance between x_next and x_current\n",
    "        tolerance = sp.sqrt(sum((x_n - x_c)**2 for x_n, x_c in zip(x_next, x_current)))\n",
    "\n",
    "        # Check for convergence\n",
    "        if tolerance < tol:\n",
    "            break\n",
    "\n",
    "        x_current = [sp.Float(val, precision) for val in x_next]\n",
    "        iterations += 1\n",
    "        \n",
    "        # Append iteration data to the list\n",
    "        iterations_data.append({\"itr\": iterations, \"x1\": x_current[0], \"x2\": x_current[1], \"tol\": tolerance})\n",
    "\n",
    "    return pd.DataFrame(iterations_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    x1, x2 = sp.symbols('x1 x2') # Define symbols   \n",
    "    x = [x1, x2] # Define variables\n",
    "    \n",
    "    f = (x1 - 1)**2 + (x2 - 2)**2  # Define function \n",
    "    x0 = [2, 2] # Initial guess \n",
    "    step_size = 0.1 # Step size\n",
    "    tol = 1e-3 # Tolerance for convergence\n",
    "    max_iter = 10 # Maximum number of iterations\n",
    "    precision = 4  # Precision (number of decimal places)\n",
    "\n",
    "    # Perform steepest descent optimization and get iteration data\n",
    "    iteration_data = steepest_descent(f, x, x0, step_size, tol, max_iter, precision)\n",
    "\n",
    "    print(iteration_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    itr       x1       x2       x3                   tol\n",
      "0     0        2        2        2                  None\n",
      "1     1  1.80000  2.00000  2.20000     0.282842712474619\n",
      "2     2  1.64000  2.00000  2.36000     0.226274190210185\n",
      "3     3  1.51200  2.00000  2.48800     0.181019365655150\n",
      "4     4  1.40960  2.00000  2.59040     0.144815458806635\n",
      "5     5  1.32768  2.00000  2.67232     0.115852380532316\n",
      "6     6  1.26214  2.00000  2.73786    0.0926818976823475\n",
      "7     7  1.20972  2.00000  2.79028    0.0741455114023852\n",
      "8     8  1.16777  2.00000  2.83223    0.0593164293523987\n",
      "9     9  1.13422  2.00000  2.86578    0.0474531367384276\n",
      "10   10  1.10737  2.00000  2.89263    0.0379625093907325\n",
      "11   11  1.08590  2.00000  2.91410    0.0303700075125860\n",
      "12   12  1.06872  2.00000  2.93128    0.0242960060100687\n",
      "13   13  1.05498  2.00000  2.94502    0.0194367778340720\n",
      "14   14  1.04398  2.00000  2.95602    0.0155494222672577\n",
      "15   15  1.03518  2.00000  2.96482    0.0124395580443390\n",
      "16   16  1.02815  2.00000  2.97185   0.00995164643543477\n",
      "17   17  1.02252  2.00000  2.97748   0.00796133737890640\n",
      "18   18  1.01801  2.00000  2.98199   0.00636906315966151\n",
      "19   19  1.01441  2.00000  2.98559   0.00509525052765768\n",
      "20   20  1.01153  2.00000  2.98847   0.00407617344814311\n",
      "21   21  1.00922  2.00000  2.99078   0.00326091852820153\n",
      "22   22  1.00738  2.00000  2.99262   0.00260872133630204\n",
      "23   23  1.00590  2.00000  2.99410   0.00208701078609532\n",
      "24   24  1.00472  2.00000  2.99528   0.00166958839851152\n",
      "25   25  1.00378  2.00000  2.99622   0.00133565048847516\n",
      "26   26  1.00302  2.00000  2.99698   0.00106850690557630\n",
      "27   27  1.00242  2.00000  2.99758  0.000854839240902255\n",
      "28   28  1.00193  2.00000  2.99807  0.000683857908523292\n",
      "29   29  1.00155  2.00000  2.99845  0.000547099815306226\n",
      "30   30  1.00124  2.00000  2.99876  0.000437686593714688\n",
      "31   31  1.00099  2.00000  2.99901  0.000350156019051882\n",
      "32   32  1.00079  2.00000  2.99921  0.000280124813942900\n",
      "33   33  1.00063  2.00000  2.99937  0.000224120084177886\n",
      "34   34  1.00051  2.00000  2.99949  0.000179309564985195\n",
      "35   35  1.00041  2.00000  2.99959  0.000143434170704371\n",
      "36   36  1.00032  2.00000  2.99968  0.000114740585340234\n",
      "37   37  1.00026  2.00000  2.99974   9.18127009874629e-5\n",
      "38   38  1.00021  2.00000  2.99979   7.34703940610744e-5\n",
      "39   39  1.00017  2.00000  2.99983   5.87695752359385e-5\n",
      "40   40  1.00013  2.00000  2.99987   4.70358950232740e-5\n",
      "41   41  1.00011  2.00000  2.99989   3.76287063504006e-5\n",
      "42   42  1.00009  2.00000  2.99992   3.00759910972798e-5\n",
      "43   43  1.00007  2.00000  2.99993   2.40405860353356e-5\n",
      "44   44  1.00005  2.00000  2.99995   1.92190812277034e-5\n",
      "45   45  1.00004  2.00000  2.99996   1.54089247054651e-5\n",
      "46   46  1.00003  2.00000  2.99997   1.23069259534212e-5\n",
      "47   47  1.00003  2.00000  2.99997   9.84550381205064e-6\n",
      "48   48  1.00002  2.00000  2.99998   7.85624491714020e-6\n",
      "49   49  1.00002  2.00000  2.99998   6.27181360150797e-6\n",
      "50   50  1.00001  2.00000  2.99999   5.02401748725129e-6\n",
      "51   51  1.00001  2.00000  2.99999   4.04609745704583e-6\n",
      "52   52  1.00001  2.00000  2.99999   3.23687796551105e-6\n",
      "53   53  1.00001  2.00000  2.99999   2.56252838933675e-6\n",
      "54   54  1.00001  2.00000  2.99999   2.02304872852291e-6\n",
      "55   55  1.00000  2.00000  3.00000   1.61843898291254e-6\n",
      "56   56  1.00000  2.00000  3.00000   1.31541387785082e-6\n",
      "57   57  1.00000  2.00000  3.00000   1.04578553184240e-6\n"
     ]
    }
   ],
   "source": [
    "# steepest decent method with three variables\n",
    "\n",
    "import sympy as sp\n",
    "import pandas as pd\n",
    "\n",
    "def steepest_descent(func, x, x0, step_size, tol, max_iter, precision):\n",
    "    \n",
    "    gradient = [sp.diff(func, var) for var in x]\n",
    "    \n",
    "    # Initialize iteration data list with initial assumption\n",
    "    iterations_data = [{\"itr\": 0, \"x1\": x0[0], \"x2\": x0[1], \"x3\": x0[2], \"tol\": None}]\n",
    "\n",
    "    x_current = x0\n",
    "    iterations = 0\n",
    "\n",
    "    while iterations < max_iter:\n",
    "        gradient_at_x = [g.subs(zip(x, x_current)) for g in gradient]\n",
    "\n",
    "        # Update rule: x_next = x_current - step_size * gradient\n",
    "        x_next = [x_curr - step_size * grad for x_curr, grad in zip(x_current, gradient_at_x)]\n",
    "\n",
    "        # Calculate the Euclidean distance between x_next and x_current\n",
    "        tolerance = sp.sqrt(sum((x_n - x_c)**2 for x_n, x_c in zip(x_next, x_current)))\n",
    "\n",
    "        # Check for convergence\n",
    "        if tolerance < tol:\n",
    "            break\n",
    "\n",
    "        x_current = [sp.Float(val, precision) for val in x_next]\n",
    "        iterations += 1\n",
    "        \n",
    "        # Append iteration data to the list\n",
    "        iterations_data.append({\"itr\": iterations, \"x1\": x_current[0], \"x2\": x_current[1], \"x3\": x_current[2], \"tol\": tolerance})\n",
    "\n",
    "    return pd.DataFrame(iterations_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    x1, x2, x3 = sp.symbols('x1 x2 x3') # Define symbols   \n",
    "    x = [x1, x2, x3] # Define variables\n",
    "    \n",
    "    f = (x1 - 1)**2 + (x2 - 2)**2 + (x3 - 3)**2  # Define function \n",
    "    x0 = [2, 2, 2] # Initial guess \n",
    "    step_size = 0.1 # Step size\n",
    "    tol = 1e-6 # Tolerance for convergence\n",
    "    max_iter = 1000 # Maximum number of iterations\n",
    "    precision = 6  # Precision (number of decimal places)\n",
    "\n",
    "    # Perform steepest descent optimization and get iteration data\n",
    "    iteration_data = steepest_descent(f, x, x0, step_size, tol, max_iter, precision)\n",
    "\n",
    "    print(iteration_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T11:50:36.332671Z",
     "start_time": "2024-03-22T11:50:36.327483Z"
    }
   },
   "id": "637e4094d4a7ff84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b057ad01d3614884"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
