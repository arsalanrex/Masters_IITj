{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal parameters: [-1.23150062e+197 -4.71955879e+197]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gradient_descent(X, y, alpha, max_iterations=1000, tol=1e-6):\n",
    "    \"\"\"\n",
    "    Gradient Descent algorithm for linear regression.\n",
    "\n",
    "    Parameters:\n",
    "        X : numpy array\n",
    "            Feature matrix with shape (m, d+1) where m is the number of samples and d is the number of features.\n",
    "            The first column should be filled with 1s for the bias term.\n",
    "        y : numpy array\n",
    "            Target values with shape (m,).\n",
    "        alpha : float\n",
    "            Learning rate.\n",
    "        max_iterations : int, optional\n",
    "            Maximum number of iterations, defaults to 1000.\n",
    "        tol : float, optional\n",
    "            Tolerance value for convergence, defaults to 1e-6.\n",
    "\n",
    "    Returns:\n",
    "        theta : numpy array\n",
    "            Optimal parameters.\n",
    "    \"\"\"\n",
    "    m, d = X.shape\n",
    "    theta = np.zeros(d)  # Initialize parameters\n",
    "    converged = False\n",
    "    iterations = 0\n",
    "    \n",
    "    while not converged and iterations < max_iterations:\n",
    "        theta_new = np.copy(theta)\n",
    "        for j in range(d):\n",
    "            gradient = alpha * np.sum((y - np.dot(X, theta)) * X[:, j])\n",
    "            theta_new[j] -= gradient\n",
    "        if np.linalg.norm(theta_new - theta) < tol:\n",
    "            converged = True\n",
    "        theta = theta_new\n",
    "        iterations += 1\n",
    "    \n",
    "    return theta\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset\n",
    "    X = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\n",
    "    y = np.array([2, 3, 4, 5])\n",
    "    \n",
    "    # Learning rate\n",
    "    alpha = 0.01\n",
    "    \n",
    "    # Running Gradient Descent\n",
    "    theta = gradient_descent(X, y, alpha)\n",
    "    print(\"Optimal parameters:\", theta)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T14:01:01.760649Z",
     "start_time": "2024-03-22T14:01:01.731011Z"
    }
   },
   "id": "5766c108a839986a"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Results:\n",
      "          Theta0        Theta1\n",
      "0   0.000000e+00  0.000000e+00\n",
      "1  -1.400000e-01 -5.400000e-01\n",
      "2  -3.610000e-01 -1.391000e+00\n",
      "3  -7.100000e-01 -2.733000e+00\n",
      "4  -1.261000e+00 -4.848000e+00\n",
      "5  -2.131000e+00 -8.183000e+00\n",
      "6  -3.502000e+00 -1.344000e+01\n",
      "7  -5.663000e+00 -2.172800e+01\n",
      "8  -9.072000e+00 -3.479400e+01\n",
      "9  -1.444600e+01 -5.539300e+01\n",
      "10 -2.291800e+01 -8.786700e+01\n",
      "11 -3.627700e+01 -1.390640e+02\n",
      "12 -5.733700e+01 -2.197770e+02\n",
      "13 -9.053900e+01 -3.470240e+02\n",
      "14 -1.428840e+02 -5.476320e+02\n",
      "15 -2.254080e+02 -8.638970e+02\n",
      "16 -3.555100e+02 -1.362499e+03\n",
      "17 -5.606200e+02 -2.148560e+03\n",
      "18 -8.839830e+02 -3.387809e+03\n",
      "19 -1.393775e+03 -5.341523e+03\n",
      "20 -2.197480e+03 -8.421614e+03\n",
      "21 -3.464545e+03 -1.327747e+04\n",
      "22 -5.462113e+03 -2.093288e+04\n",
      "23 -8.611341e+03 -3.300188e+04\n",
      "24 -1.357620e+04 -5.202902e+04\n",
      "25 -2.140345e+04 -8.202590e+04\n",
      "26 -3.374335e+04 -1.293169e+05\n",
      "27 -5.319759e+04 -2.038726e+05\n",
      "28 -8.386781e+04 -3.214121e+05\n",
      "29 -1.322204e+05 -5.067166e+05\n",
      "30 -2.084496e+05 -7.988550e+05\n",
      "31 -3.286275e+05 -1.259420e+06\n",
      "32 -5.180915e+05 -1.985516e+06\n",
      "33 -8.167875e+05 -3.130227e+06\n",
      "34 -1.287691e+06 -4.934901e+06\n",
      "35 -2.030085e+06 -7.780024e+06\n",
      "36 -3.200492e+06 -1.226545e+07\n",
      "37 -5.045675e+06 -1.933686e+07\n",
      "38 -7.954662e+06 -3.048516e+07\n",
      "39 -1.254077e+07 -4.806080e+07\n",
      "40 -1.977092e+07 -7.576935e+07\n",
      "41 -3.116946e+07 -1.194527e+08\n",
      "42 -4.913962e+07 -1.883209e+08\n",
      "43 -7.747013e+07 -2.968938e+08\n",
      "44 -1.221341e+08 -4.680622e+08\n",
      "45 -1.925481e+08 -7.379146e+08\n",
      "46 -3.035581e+08 -1.163345e+09\n",
      "47 -4.785688e+08 -1.834050e+09\n",
      "48 -7.544785e+08 -2.891436e+09\n",
      "49 -1.189459e+09 -4.558439e+09\n",
      "50 -1.875218e+09 -7.186520e+09\n",
      "Optimal Parameters: [-1.87521846e+09 -7.18651994e+09]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def gradient_descent(X, y, alpha, tol=1e-6, max_iter=50, precision=4):\n",
    "    \"\"\"\n",
    "    Gradient Descent algorithm for linear regression.\n",
    "\n",
    "    Parameters:\n",
    "        X : numpy array\n",
    "            Feature matrix with shape (m, d+1) where m is the number of samples and d is the number of features.\n",
    "            The first column should be filled with 1s for the bias term.\n",
    "        y : numpy array\n",
    "            Target values with shape (m,).\n",
    "        alpha : float\n",
    "            Learning rate.\n",
    "        tol : float, optional\n",
    "            Tolerance value for convergence, defaults to 1e-6.\n",
    "        max_iter : int, optional\n",
    "            Maximum number of iterations, defaults to 50.\n",
    "        precision : int, optional\n",
    "            Precision (number of decimal places), defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "        theta_optimal : numpy array\n",
    "            Optimal parameters.\n",
    "        theta_history : list of numpy arrays\n",
    "            List of parameter vectors at each iteration.\n",
    "    \"\"\"\n",
    "    m, d = X.shape\n",
    "    theta = np.zeros(d)  # Initialize parameters\n",
    "    theta_history = [theta.copy()]  # Store initial parameters\n",
    "    iterations = 0\n",
    "    \n",
    "    while iterations < max_iter:\n",
    "        theta_new = np.copy(theta)\n",
    "        for j in range(d):\n",
    "            gradient = alpha * np.sum((y - np.dot(X, theta)) * X[:, j])\n",
    "            theta_new[j] -= gradient\n",
    "        if np.linalg.norm(theta_new - theta) < tol:\n",
    "            break\n",
    "        theta = theta_new\n",
    "        theta_history.append(theta.copy())  # Store parameters at this iteration\n",
    "        iterations += 1\n",
    "    \n",
    "    theta_optimal = np.round(theta, precision)\n",
    "    \n",
    "    return theta_optimal, theta_history\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Example dataset\n",
    "    X = np.array([[1, 2], [1, 3], [1, 4], [1, 5]])\n",
    "    y = np.array([2, 3, 4, 5])\n",
    "    \n",
    "    # Learning rate\n",
    "    alpha = 0.01\n",
    "    \n",
    "    # Additional inputs\n",
    "    tol = 1e-3\n",
    "    max_iter = 50\n",
    "    precision = 3\n",
    "    \n",
    "    # Running Gradient Descent\n",
    "    theta_optimal, theta_history = gradient_descent(X, y, alpha, tol=tol, max_iter=max_iter, precision=precision)\n",
    "    \n",
    "    # Create a DataFrame to store iteration results\n",
    "    iteration_results = pd.DataFrame(columns=[f'Theta{i}' for i in range(len(theta_history[0]))])\n",
    "    for i, theta in enumerate(theta_history):\n",
    "        iteration_results.loc[i] = np.round(theta, precision)\n",
    "    \n",
    "    print(\"Iteration Results:\")\n",
    "    print(iteration_results)\n",
    "    \n",
    "    print(\"Optimal Parameters:\", theta_optimal)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T14:01:22.786929Z",
     "start_time": "2024-03-22T14:01:22.758140Z"
    }
   },
   "id": "4d2b8cad398df5f6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "ce65b65070863fb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
