{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Concatenate\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Parameters\n",
    "INPUT_SIZE = (256, 256)\n",
    "BS = 16\n",
    "ROOT_DIR = \"/kaggle/\"\n",
    "\n",
    "# Paths for various terrain types\n",
    "#DATASETS = [\"agri\", \"barrenland\", \"grassland\", \"urban\"]\n",
    "DATASETS = [\"agri\"]\n",
    "ROOT_DATASET_PATH = os.path.join(ROOT_DIR, 'input/sentinel12-image-pairs-segregated-by-terrain/v_2')\n",
    "\n",
    "# Set up the data directories\n",
    "DATA_GEN_INPUT = os.path.join(ROOT_DIR, 'DATASET')\n",
    "if os.path.exists(DATA_GEN_INPUT):\n",
    "    shutil.rmtree(DATA_GEN_INPUT)\n",
    "os.mkdir(DATA_GEN_INPUT)\n",
    "\n",
    "# Link all the datasets into the main DATASET directory\n",
    "for terrain in DATASETS:\n",
    "    src_s1 = os.path.join(ROOT_DATASET_PATH, terrain, \"s1\")\n",
    "    src_s2 = os.path.join(ROOT_DATASET_PATH, terrain, \"s2\")\n",
    "    dst_s1 = os.path.join(DATA_GEN_INPUT, f\"DATA_{terrain}_s1\")\n",
    "    dst_s2 = os.path.join(DATA_GEN_INPUT, f\"DATA_{terrain}_s2\")\n",
    "    os.symlink(src_s1, dst_s1)\n",
    "    os.symlink(src_s2, dst_s2)\n",
    "    print(f\"Linked {src_s1} to {dst_s1}\")\n",
    "    print(f\"Linked {src_s2} to {dst_s2}\")\n",
    "\n",
    "# Image preprocessing function\n",
    "def preprocessing_function(img):\n",
    "    return np.float32(img / 255.0)\n",
    "\n",
    "# Custom data generator\n",
    "class PairedImageDataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, base_dir, terrain_types, batch_size, input_size):\n",
    "        self.base_dir = base_dir\n",
    "        self.terrain_types = terrain_types\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        self.image_files = self._get_image_files()\n",
    "        print(f\"Total image pairs: {len(self.image_files)}\")\n",
    "\n",
    "    def _get_image_files(self):\n",
    "        files = []\n",
    "        for terrain in self.terrain_types:\n",
    "            s1_dir = os.path.join(self.base_dir, f\"DATA_{terrain}_s1\")\n",
    "            s2_dir = os.path.join(self.base_dir, f\"DATA_{terrain}_s2\")\n",
    "            s1_files = sorted(os.listdir(s1_dir))\n",
    "            s2_files = sorted(os.listdir(s2_dir))\n",
    "            print(f\"Found {len(s1_files)} images for {terrain}\")\n",
    "            files.extend([(os.path.join(s1_dir, s1), os.path.join(s2_dir, s2)) \n",
    "                          for s1, s2 in zip(s1_files, s2_files)])\n",
    "        return files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_files = self.image_files[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        x_batch = np.zeros((self.batch_size, *self.input_size, 1))\n",
    "        y_batch = np.zeros((self.batch_size, *self.input_size, 3))\n",
    "\n",
    "        for i, (s1_file, s2_file) in enumerate(batch_files):\n",
    "            try:\n",
    "                s1_img = tf.keras.preprocessing.image.load_img(s1_file, color_mode='grayscale', target_size=self.input_size)\n",
    "                s2_img = tf.keras.preprocessing.image.load_img(s2_file, color_mode='rgb', target_size=self.input_size)\n",
    "                \n",
    "                s1_array = tf.keras.preprocessing.image.img_to_array(s1_img)\n",
    "                s2_array = tf.keras.preprocessing.image.img_to_array(s2_img)\n",
    "                \n",
    "                x_batch[i, :, :, 0] = preprocessing_function(s1_array[:, :, 0])\n",
    "                y_batch[i] = preprocessing_function(s2_array)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading image pair: {s1_file}, {s2_file}\")\n",
    "                print(f\"Error message: {str(e)}\")\n",
    "\n",
    "        return x_batch, y_batch\n",
    "\n",
    "# Create data generator\n",
    "train_generator = PairedImageDataGenerator(DATA_GEN_INPUT, DATASETS, BS, INPUT_SIZE)\n",
    "\n",
    "# Define the model using VGG16 for transfer learning\n",
    "def create_model(input_shape=(256, 256, 1)):\n",
    "    # Encoder (VGG16)\n",
    "    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "    vgg16.trainable = False\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = Input(shape=input_shape)\n",
    "    \n",
    "    # Convert grayscale to 3-channel\n",
    "    x = Conv2D(3, (1, 1), padding='same')(input_layer)\n",
    "    \n",
    "    # Extract features from VGG16\n",
    "    features = vgg16(x)\n",
    "    \n",
    "    # Decoder\n",
    "    x = Conv2D(256, (3, 3), activation='relu', padding='same')(features)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = UpSampling2D((2, 2))(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output_layer = Conv2D(3, (3, 3), activation='tanh', padding='same')(x)\n",
    "    \n",
    "    return Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Define custom loss function\n",
    "def custom_loss():\n",
    "    vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "    vgg16.trainable = False\n",
    "    def loss(y_true, y_pred):\n",
    "        # Combine MSE and perceptual loss\n",
    "        mse = tf.keras.losses.MeanSquaredError()(y_true, y_pred)\n",
    "        perceptual = tf.reduce_mean(tf.square(vgg16(y_true) - vgg16(y_pred)))\n",
    "        return mse + 0.1 * perceptual\n",
    "    return loss\n",
    "\n",
    "# Metrics calculation\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    mse = tf.keras.losses.MeanSquaredError()(y_true, y_pred)\n",
    "    ssim = tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))\n",
    "    psnr = tf.reduce_mean(tf.image.psnr(y_true, y_pred, max_val=1.0))\n",
    "    return mse, ssim, psnr\n",
    "\n",
    "# Function to visualize and test model performance\n",
    "def test_model(data_generator, model):\n",
    "    sar_img, true_color = next(iter(data_generator))\n",
    "    \n",
    "    # Prepare for prediction\n",
    "    colorized_img = model.predict(sar_img)\n",
    "\n",
    "    # Plot images\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    ax[0].imshow(sar_img[0, :, :, 0], cmap='gray')\n",
    "    ax[0].set_title('Original SAR Image (s1)')\n",
    "    ax[1].imshow(np.clip(colorized_img[0], 0, 1))\n",
    "    ax[1].set_title('Colorized SAR Image')\n",
    "    ax[2].imshow(true_color[0])\n",
    "    ax[2].set_title('True Color Image (s2)')\n",
    "    plt.show()\n",
    "\n",
    "# Build and compile the model\n",
    "model = create_model(list(INPUT_SIZE) + [1])\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "model.compile(optimizer=opt, loss=custom_loss(), metrics=[compute_metrics])\n",
    "\n",
    "# Early stopping criteria\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "EPOCHS = 100\n",
    "train_loss = []\n",
    "\n",
    "# Training loop with early stopping and visual testing\n",
    "for epoch in range(0, EPOCHS):\n",
    "    print(f\"[INFO] Starting epoch {epoch + 1}/{EPOCHS}\")\n",
    "    epoch_start = time.time()\n",
    "    loss_batch = []\n",
    "\n",
    "    # Training loop for one epoch\n",
    "    for i in tqdm(range(len(train_generator))):\n",
    "        try:\n",
    "            sar_data, true_color = train_generator[i]\n",
    "            \n",
    "            # Train model\n",
    "            loss = model.train_on_batch(sar_data, true_color)\n",
    "            loss_batch.append(loss)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during training on batch {i}\")\n",
    "            print(f\"Error message: {str(e)}\")\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    if loss_batch:\n",
    "        avg_loss = np.mean(loss_batch, axis=0)\n",
    "        train_loss.append(avg_loss)\n",
    "        loss_names = ['Total Loss', 'MSE', 'SSIM', 'PSNR']\n",
    "        for name, value in zip(loss_names, avg_loss):\n",
    "            print(f'{name}: {value:.4f}')\n",
    "\n",
    "        # Early stopping logic\n",
    "        if avg_loss[0] < best_val_loss - 0.001:\n",
    "            best_val_loss = avg_loss[0]\n",
    "            epochs_without_improvement = 0\n",
    "        else:\n",
    "            epochs_without_improvement += 1\n",
    "\n",
    "        if epochs_without_improvement > patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        # Visualize colorization results every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(train_loss)\n",
    "            plt.legend(['Total Loss', 'MSE', 'SSIM', 'PSNR'])\n",
    "            plt.title(f'Training Loss - Epoch {epoch + 1}')\n",
    "            plt.show()\n",
    "            test_model(train_generator, model)\n",
    "    else:\n",
    "        print(\"No valid batches in this epoch\")\n",
    "\n",
    "    epoch_end = time.time()\n",
    "    elapsed = (epoch_end - epoch_start) / 60.0\n",
    "    print(f\"Epoch took {elapsed:.4f} minutes\")\n",
    "\n",
    "# Save the model\n",
    "model.save('sar_colorization_model.h5')\n",
    "print(\"Model saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
